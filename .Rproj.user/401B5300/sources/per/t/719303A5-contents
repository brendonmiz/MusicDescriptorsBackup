---
title: "musicdesc_scratchpad"
author: "Brendon Mizener"
date: "3/15/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



  Fewer studies have used adjectives to evaluate music holistically (examples include @Wedin1969, @Wedin1972, @Hevner1936, @Canazza2001].  
  Other studies exist that evaluate how various technical aspects of music correspond to emotions for the purpose of induction, (see @Bruner1990 for an early summary) but the musical characteristics don't accurately capture the full dimensionality that composers consider when writing music. Also, many of the studies that investigate from this perspective impose strict limitations on how the stimuli vary, which is useful for illuminating very specific effects of a single musical element or characteristic, but makes it impossible to evaluate interactions between any musical variables. Assessing the interplay between the technical aspects of music and descriptive/affective requires a fine - grained approach that is able to evaluate the correlations and covariates between many dimensions simultaneously. Imaging studies that evaluate the neural response to music could be very valuable here, but they require prohibitive amounts of resources, are severely location-dependent. That being said, there are multivariate techniques that are commonly used in imaging studies that are designed for the specific purpose of what we aim with this study.
  Few studies using multivariate analyses to evaluate music holistically have used all four: large numbers of stimuli, large numbers of descriptors, and a large sample size, and ecologically valid musical stimuli. @Canazza2001, for example, only used a single stimulus. @Wedin1972 used a large number of stimuli (35), a large number of 'attributes' (150), but a sample size of only 49.
  One way to minimize the individual associations is to use novel music, but that requires controlling for familiarity. In a musically trained population, the easiest way to do that is to compose novel music, so we composed 30 novel stimuli for this specific study.  
A recent study  

  This work should be considered 


  Research on the perception of timbre has provided some useful templates in navigating this space. @Wallmark2018 notes that because timbre is not a linear phenomenon, as tempo or pitch is, qualitative descriptors make the most sense when evaluating it. A similar conceptual framework would ideally be applied to music to be evaluated holistically. In this regard, we consider the current work to be theoretically distinct from works that specifically evaluate emotional perception in music. The present work does confirm the results of those studies thanks to the appearance of the valence-arousal plane. However, we did not intentionally use emotionally valenced adjectives when selecting the vocabulary participants used to rate stimuli. Additionally, we compare the descriptors to music-theoretical dimensions that are rated on discrete scales in order to evaluate which elements from either set of descriptors covary with one another.
  Another specific reference in the study of timbre that provides a useful comparison is @Zacharakis2014 This study addresses not only the qualitative nature of analysis, but serves as an interesting guidepost in terms of intercultural or inter-lingual music evaluation. @Zacharakis2014 found that for two different languages, meaning was largely 

  
  
  - we used correspondence analysis inste4ad of a nother form of factor or multivariate analysis, which allows for biplots. this will allow you to see not only where the adjctives lie in the factor space4, but also where the musical descriptors/quialities lie in atheir own factor space. While  the biplot specifically then allows you to plot the excerpts in the same space as the descriptors, which provides a clear, quick, visual reference for what excerpts or musical pieces fall in to what space of the cognitive space. 


 By developing investigative paradigms that are accessible on mobile platforms, and that reduce participant demand as much as possible while maintaining rigor, we are likely to be able to reach a much greater subset of the population. If we are able to pair this kind of data gathering with appropriate exploratory analysis, we can target much more effectively where we might investigate with more traditional hypothesis testing.
  The initial motivation for this paper was as precursor work to a study aimed at evaluating the cross-modal similarities between gustatory perception and auditory perception. There exist various established gustatory 'spaces' onto which music perception might be mapped, for example in wine tasting [wine citations here], the only analogous space in music listening deals specifically with emotional processing. The goal of this study was to evaluate whether or not there is a similar music perception space against which other sensory domains could be compared, and to select stimuli that could anchor the corners of such a space. 




In this study, we attempt to address three specific issues with the field as a whole: sample & size, mode of investigation, and balancing  analysis. The gradual increase in complexity of studies in behavior and cognition, coupled with the rise of questions about the universality of experience and the democratization of science, compels us to find novel ways of investigating the experience of music. 










###################
  Much of the work in music cognition focused on adjectives or descriptors has focused on timbre. [citations: Wallmark, Zacharakis] Early work attempting this: @Hevner1936a

###################
  The current study aimed to evaluate a set of musical stimuli in two ways, by association with a list of adjectives, and by rating on a set of musical qualities. These will then be analyzed independently and then combined to see what effects can be deduced from the combination of the two.

###################
  Valence/arousal model


Existing multivariate approaches: 
@Roda2014a: uses clustering, not descriptors, compares directly to physical features of music. Attempts to quantify higher dimensions
@Droit-Volet2013a: uses point-scale ratings (unpleasant/pleasant; calm/exciting) and investigates effect of time on ratings.


  - as questions become more complex, the burden on researchers to define parameters in which to test and evaluate their results becomes much greater. Controlling for extraneous variables becomes a problem in and of itself.
  - lacks generalizability because of narrowness
  - rising question of universalities & cross-cultural perception in music 
  - how do we access a larger population

  - This all results in a need for an experimental paradigm that is robust to violations in experimental procedure, accesses 

  - After cleaning and preprocessing, the data for each participant will take the form of a pseudo contingency table. The difference here is that a contingency table is specifically when a participant selects only one option from a list for each stimulus, resulting in a table with one and only one one (1) per row. Because we are using the CATA technique, a one (1) at the intersection of each row or column indicates that the participant selected that adjective or musical quality for that stimulus. A zero means that they did not. These individual tables are all compiled into a 'brick', or three-dimensional array of data with Observations (stimuli) on the rows, variables (musical qualities or adjectives) on the columns, and participants on the third dimension, which we will refer to as 'pages' here. This brick is then summed across pages into a single table, so that any given cell contains the total number of times a participant selected a given adjective or quality to match with a stimulus. 
  From this point there are two sets of data that can be analyzed. The first is the 3D array, which can be analyzed using various distance analyses, to evaluate differences between the participants using grouping variables extracted from the demographics surveys. The other is the pseudo contingency table, which can be analyzed using various multivariate techniques.
  - what processing steps are needed



## Present questions & methods of analysis
  The initial motivation for this study came from a cross-modal study investigating cross modal sensory mapping between gustation perception, specifically beer, and music perception. Prior versions of this experiment (unpublished) suggest that a wide variety in musical stimuli was necessary to determine any correlations or differences. As such, this study is designed to investigate whether a music cognitive listening space can be established using this paradigm, to allow cross-modal comparison. Additional questions arise from the study itself: are there significant differences in how participants from different nationalities (and by extension musical cultures) perceive, or, more precisely, describe music? Are there parallels in how music is evaluated using music non-specific descriptors and music-specific qualities? Because this study was designed to be exploratory in nature, we feel it would be poor scientific practice to present specific hypotheses. 
  
  
  
  
```{r, fig.height=10}

load("catadatamusdim.RData")
load("adj.catadata.list.RData")
load("adjdesign.RData")
load("adjectivecolors.RData")

adjct <- adj.catadata.list$adjcontingency
quact <- catadata.list$contingency

data4mfa <- abind(adjct, quact, along = 2)

gr.vec <- data.frame(matrix(data = NA, nrow = 1, ncol = dim(data4mfa)[2]))

gr.vec[1,1:dim(adjct)[2]] <- "A"
gr.vec[1,(dim(adjct)[2]+1):(dim(adjct)[2]+dim(quact)[2])] <- "Q"

colnames(gr.vec) <- colnames(data4mfa)
rownames(gr.vec) <- "group"

mfatestres <- mpMFA(data4mfa, gr.vec)

renameCols <- function(x){
   colnames(x) <- paste("Dimension", 1:ncol(x))
   x
}

RVtestmat <- mfatestres$mexPosition.Data$InnerProduct$RVMatrix

corrplot::corrplot(RVtestmat, method = "color", addCoefasPercent = TRUE) 

Eig4testscree <- mfatestres$mexPosition.Data$Table$eigs

mfatestscree <- PlotScree(Eig4testscree, plotKaiser = TRUE)

MFA_testFMap <- createFactorMap(
                            mfatestres$mexPosition.Data$Table$fi,
                            col.points = cols$ex.oc,
                            col.labels = cols$ex.oc,
                            alpha.points = .6,
                            pch = 17,
                            cex = 5,
                            display.labels = TRUE,
                            constraints = minmaxHelper4Partial(FactorScores = mfatestres$mexPosition.Data$Table$fi,
                              partialFactorScores = mfatestres$mexPosition.Data$Table$partial.fi.array)
                            )
                            
label4testMap <- createxyLabels.gen(1,2,
                                lambda = mfatestres$mexPosition.Data$Table$eigs,
                                tau = mfatestres$mexPosition.Data$Table$t,
                                axisName = "Dimension ")




fi4testpfs <- mfatestres$mexPosition.Data$Table$fi

fi4testpfs <- renameCols(fi4testpfs)

pfi4testpfs <- mfatestres$mexPosition.Data$Table$partial.fi.array

pfi4testpfs <- renameCols(pfi4testpfs)

rownames(pfi4testpfs) <- c(1:30)

dimnames(pfi4testpfs)[[3]] <- c("A", "Q")

map4testPFS <- createPartialFactorScoresMap(
                                        factorScores = fi4testpfs,
                                        partialFactorScores = pfi4testpfs,
                                        axis1 = 1, axis2 = 2,
                                        colors4Items = cols$ex.oc,
                                        names4Partial = c("A", "Q"),
                                        font.labels = 'bold'
                                        )



                        
                                          
partialfstestmap <- MFA_testFMap$zeMap + 
                  label4testMap + 
                    map4testPFS$linesColByItems + 
                      map4testPFS$pointsColByItems + 
                        map4testPFS$labelsColByItems +
                          ggtitle("Contributions to the Excerpts Factor Scores")

print(partialfstestmap)

```
  
  