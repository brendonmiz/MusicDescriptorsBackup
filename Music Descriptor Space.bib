Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Cohn2001,
author = {Cohn, Richard and Hyer, Brian and Dahlhaus, Carl and Anderson, Julian and Wilson, Charles},
booktitle = {Grove Music Online},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Grove-Harmony.pdf:pdf},
isbn = {9781561592630},
publisher = {Oxford University Press},
title = {{Harmony}},
year = {2001}
}
@article{Trehub2015,
abstract = {Musical behaviours are universal across human populations and, at the same time, highly diverse in their structures, roles and cultural interpretations. Although laboratory studies of isolated listeners and music-makers have yielded important insights into sensorimotor and cognitive skills and their neural underpinnings, they have revealed little about the broader significance of music for individuals, peer groups and communities. This reviewpresents a sampling of musical forms and coordinated musical activity across cultures, with the aim of highlighting key similarities and differences. The focus is on scholarly and everyday ideas about music-what it is and where it originates-as well the antiquity of music and the contribution of musical behaviour to ritual activity, social organization, caregiving and group cohesion. Synchronous arousal, action synchrony and imitative behaviours are among the means by which music facilitates social bonding. The commonalities and differences in musical forms and functions across cultures suggest new directions for ethnomusicology, music cognition and neuroscience, and a pivot away from the predominant scientific focus on instrumental music in the Western European tradition.},
author = {Trehub, Sandra E. and Becker, Judith and Morley, Iain},
doi = {10.1098/rstb.2014.0096},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/rstb20140096.pdf:pdf},
issn = {14712970},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {Antiquity,Cross-cultural,Music,Ritual,Social cohesion},
number = {1664},
pmid = {25646519},
title = {{Cross-cultural perspectives on music and musicality}},
volume = {370},
year = {2015}
}
@article{Bigand2005,
abstract = {Musically trained and untrained listeners were required to listen to 27 musical excerpts and to group those that conveyed a similar emotional meaning (Experiment 1). The groupings were transformed into a matrix of emotional dissimilarity that was analysed through multidimensional scaling methods (MDS). A 3-dimensional space was found to provide a good fit of the data, with arousal and emotional valence as the primary dimensions. Experiments 2 and 3 confirmed the consistency of this 3-dimensional space using excerpts of only 1 second duration. The overall findings indicate that emotional responses to music are very stable within and between participants, and are weakly influenced by musical expertise and excerpt duration. These findings are discussed in light of a cognitive account of musical emotion. {\textcopyright} 2005 Psychology Press Ltd.},
author = {Bigand, E. and Vieillard, S. and Madurell, F. and Marozeau, J. and Dacquet, A.},
doi = {10.1080/02699930500204250},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/000103-multidimensional-scaling-of-emotional-responses-to-music-the-effect-of-musical-expertise-and-of-the-duration-of-the-excerpts.pdf:pdf},
issn = {02699931},
journal = {Cognition and Emotion},
number = {8},
pages = {1113--1139},
title = {{Multidimensional scaling of emotional responses to music: The effect of musical expertise and of the duration of the excerpts}},
volume = {19},
year = {2005}
}
@article{Zacharakis2014,
abstract = {A STUDY OF MUSICAL TIMBRE SEMANTICS WAS conducted with listeners from two different linguistic groups. In two separate experiments, native Greek and English speaking participants were asked to describe 23 musical instrument tones of variable pitch using a predefined vocabulary of 30 adjectives. The common experimental protocol facilitated the investigation ofthe influence of language on musical timbre semantics by allowing for direct comparisons between linguistic groups. Data reduction techniques applied to the data of each group revealed three salient semantic dimen- sions that shared common conceptual properties between linguistic groups namely: luminance, texture, and mass. The results supported universality of timbre semantics. A correlation analysis between physical char- acteristics and semantic dimensions associated: i) tex- ture with the energy distribution of harmonic partials, ii) thickness (a term related to either mass or lumi- nance) and brilliance with inharmonicity and spectral centroid variation, and iii) F0 with mass or luminance depending on the linguistic group},
author = {Zacharakis, Asterios and Pastiadis, Konstantinos and Reiss, Joshua D.},
doi = {10.1525/MP.2014.31.4.339},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/mp.2014.31.4.339.pdf:pdf},
journal = {Music Perception: An Interdisciplinary Journal},
keywords = {CATPCA,acoustic correlates,musical timbre semantics,timbre spaces,verbal attribute magnitude estimation},
number = {4},
pages = {339--358},
title = {{An Interlanguage Study of Musical Timbre Semantic Dimensions and Their Acoustic Correlates}},
url = {https://www.jstor.org/stable/10.1525/mp.2014.31.4.339{\%}0AJSTOR},
volume = {31},
year = {2014}
}
@incollection{Abdi2007a,
abstract = {SVD and one kind of GSVD},
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
doi = {10.4135/9781412952644.n413},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/C34-SVD and GSVD.pdf:pdf},
pages = {1--14},
publisher = {Sage},
title = {{Singular and Generalized Singular Value Decomposition}},
year = {2007}
}
@article{Wedin1969,
abstract = {style dimensions in  and by Eisler (1766) on the  of -reproducing systems the primary factor in his experiment covered dynamic aspects of the , con- nected  be known as 'Osgood's semantic differential' (Osgood, 1957; for a brief  see Osgood },
author = {Wedin, Lage},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/STM1969Wedin.pdf:pdf},
journal = {Swedish Journal of Musicology},
pages = {119--140},
title = {{Dimension Analysis of Emotional Expression in Music}},
volume = {51},
year = {1969}
}
@inproceedings{Canazza2001,
abstract = {A musical interpretation is often the result of a wide range of requirements on expressiveness rendering and technical skills. Aspects that are indicated with the term expressive intention and which refers to the communication of moods and feelings are being considered more and more important in performer-computer interaction during music performance. Recent studies demonstrated that by modifying opportunely systematic deviations introduced by the musician it is possible to convey different sensitive content like expressive intentions and emotions. We present an abstract space, that can be used for the user interface, which represents, at an abstract level, the expressive content and the interaction between the performer and the expressive engine. This space was derived by multidimensional analysis of perceptual tests on various professionally performed pieces ranging from western classical to popular music. This space reflects how the musical performances are organized in the listener's mind.},
author = {Canazza, Sergio and {De Poli}, Giovanni and Rod{\`{a}}, Antonio and Vidolin, Alvise and Zanon, Patrick},
booktitle = {Proc. of MOSART. Workshop on current research directions in computer music},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Kinematics-energy{\_}space{\_}for{\_}expressive{\_}interaction.pdf:pdf},
pages = {35--40},
title = {{Kinematics-energy space for expressive interaction in music performance}},
url = {http://www.dei.unipd.it/{~}musica/},
year = {2001}
}
@article{Watt1998,
abstract = {Music has a strong effect on people's mental state and behaviour. This effect can be at a simple motor level, or it can be at a more complex level of arbitrary association or it can be at a much more complex cognitive and representational level. This latter case is of specific interest because it implies that, to some degree, music can be said to have a content that is not musical. Thus, when music is taken to depict a rough sea, then something in the music causes that: the music contains something that is being taken to signify roughness-of-sea. The existence of a large, natural yet arbitrary vocabulary (as is demonstrably the case for language) to relate musical expressions to non-musical events/objects seems implausible. In this paper, the possibility is explored that the vocabulary of musical expression concerns psychological aspects of people. Thus music, it is hypothesised, can express male/female-ness and good/evil-ness and happy/sad-ness and so on. Data obtained in a novel paradigm designed to test this hypothesis are described. Substantial support for the hypothesis is found: the implications of this are discussed.},
author = {Watt, Roger J. and Ash, Roisin L.},
doi = {10.1177/102986499800200103},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/102986499800200103.pdf:pdf},
issn = {1029-8649},
journal = {Musicae Scientiae},
number = {1},
pages = {33--53},
title = {{A Psychological Investigation of Meaning in Music}},
volume = {2},
year = {1998}
}
@article{Altenmuller2015,
abstract = {Music listening and music making activities are powerful tools to engage multisensory and motor networks, induce changes within these networks, and foster links between distant, but functionally related brain regions with continued and life-long musical practice. These multimodal effects of music together with music's ability to tap into the emotion and reward system in the brain can be used to facilitate and enhance therapeutic approaches geared toward rehabilitating and restoring neurological dysfunctions and impairments of an acquired or congenital brain disorder. In this article, we review plastic changes in functional networks and structural components of the brain in response to short- and long-term music listening and music making activities. The specific influence of music on the developing brain is emphasized and possible transfer effects on emotional and cognitive processes are discussed. Furthermore, we present data on the potential of using musical tools and activities to support and facilitate neurorehabilitation. We will focus on interventions such as melodic intonation therapy and music-supported motor rehabilitation to showcase the effects of neurologic music therapies and discuss their underlying neural mechanisms.},
author = {Altenm{\"{u}}ller, Eckart and Schlaug, Gottfried},
doi = {10.1016/bs.pbr.2014.11.029},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S0079612314000302-main.pdf:pdf},
issn = {18757855},
journal = {Progress in Brain Research},
keywords = {Brain plasticity,Melodic intonation therapy,Music-supported training,Neurologic music therapy,Neurorehabilitation},
pages = {237--252},
pmid = {25725918},
title = {{Apollo's gift: New aspects of neurologic music therapy}},
volume = {217},
year = {2015}
}
@article{Cowen2017,
abstract = {Emotions are centered in subjective experiences that people represent, in part, with hundreds, if not thousands, of semantic terms. Claims about the distribution of reported emotional states and the boundaries between emotion categories—that is, the geometric organization of the semantic space of emotion—have sparked intense debate. Here we introduce a conceptual framework to analyze reported emotional states elicited by 2,185 short videos, examining the richest array of reported emotional experiences studied to date and the extent to which reported experiences of emotion are structured by discrete and dimensional geometries. Across self-report methods, we find that the videos reliably elicit 27 distinct varieties of reported emotional experience. Further analyses revealed that categorical labels such as amusement better capture reports of subjective experience than commonly measured affective dimensions (e.g., valence and arousal). Although reported emotional experiences are represented within a semantic space best captured by categorical labels, the boundaries between categories of emotion are fuzzy rather than discrete. By analyzing the distribution of reported emotional states we uncover gradients of emotion—from anxiety to fear to horror to disgust, calmness to aesthetic appreciation to awe, and others—that correspond to smooth variation in affective dimensions such as valence and dominance. Reported emotional states occupy a complex, high-dimensional categorical space. In addition, our library of videos and an interactive map of the emotional states they elicit (https://s3-us-west-1.amazonaws.com/emogifs/map.html) are made available to advance the science of emotion.},
author = {Cowen, Alan S. and Keltner, Dacher},
doi = {10.1073/pnas.1702247114},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/E7900.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Dimensions,Discrete emotion,Emotional experience,Semantic space},
number = {38},
pages = {E7900--E7909},
pmid = {28874542},
title = {{Self-report captures 27 distinct categories of emotion bridged by continuous gradients}},
volume = {114},
year = {2017}
}
@article{Liu2018,
abstract = {Music and speech both communicate emotional meanings in addition to their domain-specific contents. But it is not clear whether and how the two kinds of emotional meanings are linked. The present study is focused on exploring the emotional connotations of musical timbre of isolated instrument sounds through the perspective of emotional speech prosody. The stimuli were isolated instrument sounds and emotional speech prosody categorized by listeners into anger, happiness and sadness, respectively. We first analyzed the timbral features of the stimuli, which showed that relations between the three emotions were relatively consistent in those features for speech and music. The results further echo the size-code hypothesis in which different sound timbre indicates different body size projections. Then we conducted an ERP experiment using a priming paradigm with isolated instrument sounds as primes and emotional speech prosody as targets. The results showed that emotionally incongruent instrument-speech pairs triggered a larger N400 response than emotionally congruent pairs. Taken together, this is the first study to provide evidence that the timbre of simple and isolated musical instrument sounds can convey emotion in a way similar to emotional speech prosody.},
author = {Liu, Xiaoluan and Xu, Yi and Alter, Kai and Tuomainen, Jyrki},
doi = {10.3389/fpsyg.2018.00737},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/fpsyg-09-00737.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {ERP,Emotion,Emotional speech prosody,Musical timbre,N400},
number = {MAY},
pages = {1--10},
title = {{Emotional connotations of musical instrument timbre in comparison with emotional speech prosody: Evidence from acoustics and event-related potentials}},
volume = {9},
year = {2018}
}
@article{Panda2020,
abstract = {This work advances the music emotion recognition state-of-the-art by proposing novel emotionally-relevant audio features. We reviewed the existing audio features implemented in well-known frameworks and their relationships with the eight commonly defined musical concepts. This knowledge helped uncover musical concepts lacking computational extractors, to which we propose algorithms - namely related with musical texture and expressive techniques. To evaluate our work, we created a public dataset of 900 audio clips, with subjective annotations following Russell's emotion quadrants. The existent audio features (baseline) and the proposed features (novel) were tested using 20 repetitions of 10-fold cross-validation. Adding the proposed features improved the F1-score to 76.4 percent (by 9 percent), when compared to a similar number of baseline-only features. Moreover, analysing the features relevance and results uncovered interesting relations, namely the weight of specific features and musical concepts to each emotion quadrant, and warrant promising new directions for future research in the field of music emotion recognition, interactive media, and novel music interfaces.},
author = {Panda, Renato and Malheiro, Ricardo and Paiva, Rui Pedro},
doi = {10.1109/TAFFC.2018.2820691},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/08327886.pdf:pdf},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Affective computing,audio databases,emotion recognition,feature extraction,music information retrieval},
number = {4},
pages = {614--626},
title = {{Novel Audio Features for Music Emotion Recognition}},
volume = {11},
year = {2020}
}
@article{Raman2017,
abstract = {MODULATION, A SHIFT IN MODE (R ¯AGAM), IS important in South Indian classical (Carn¯atic) music. Here we investigate the sensitivity ofCarn¯atic andWest- ern listeners to such shifts. Carn¯atic music has two kinds of shifts: r¯agam¯alik¯a (retaining tonal center, resembling a shift from C major to C minor in Western music) and grahab¯edham (shifting tonal center, resem- bling a shift from C major to A minor). Listeners heard modulating pieces of music and indicated the point of modulation, and were measured for accuracy and latency. Indians were more accurate than Westerners with both types of modulation but Westerners were faster with grahab¯edhams. Cues could explain perfor- mance differences between nationalities: Indians were more familiar with r¯agam¯alik¯a-type modulations whereas Westerners' culture made them more familiar with grahab¯edham-type modulations. Increased caution toward the less familiar grahab¯edhams for Indians could explain their slower response time compared to r¯agam¯alik¯as. With grahab¯edhams, hit rates for both groups were comparably high, but Westerners' lower level of accuracy was due to higher false-alarm rates to lures that were superficially similar to actual modula- tions. This indicated their dependence on surface-level cues in the absence of familiarity and culture-specific information. Music training helped teachers in both groups make fewer errors when compared to students. Older listeners' performance was comparable to that of younger listeners. Received:},
author = {Raman, Rachna and Dowling, W. Jay},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Raman, Dowling - 2017 - Perception of modulations in south indian classical (carnatic) music by student and teacher musicians A cross-cu.pdf:pdf},
journal = {Music Perception},
keywords = {culture-specific cues,grahab},
number = {4},
pages = {424--437},
title = {{Perception of modulations in south indian classical (carnatic) music by student and teacher musicians: A cross-cultural study}},
volume = {34},
year = {2017}
}
@article{Purwins2008,
abstract = {In Part I [Purwins H, Herrera P, Grachten M, Hazan A, Marxer R, Serra X. Computational models of music perception and cognition I: The perceptual and cognitive processing chain. Physics of Life Reviews 2008, in press, doi:10.1016/j.plrev.2008.03.004], we addressed the study of cognitive processes that underlie auditory perception of music, and their neural correlates. The aim of the present paper is to summarize empirical findings from music cognition research that are relevant to three prominent music theoretic domains: rhythm, melody, and tonality. Attention is paid to how cognitive processes like category formation, stimulus grouping, and expectation can account for the music theoretic key concepts in these domains, such as beat, meter, voice, consonance. We give an overview of computational models that have been proposed in the literature for a variety of music processing tasks related to rhythm, melody, and tonality. Although the present state-of-the-art in computational modeling of music cognition definitely provides valuable resources for testing specific hypotheses and theories, we observe the need for models that integrate the various aspects of music perception and cognition into a single framework. Such models should be able to account for aspects that until now have only rarely been addressed in computational models of music cognition, like the active nature of perception and the development of cognitive capacities from infancy to adulthood. {\textcopyright} 2008.},
author = {Purwins, Hendrik and Grachten, Maarten and Herrera, Perfecto and Hazan, Amaury and Marxer, Ricard and Serra, Xavier},
doi = {10.1016/j.plrev.2008.03.005},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S157106450800016X-main.pdf:pdf},
issn = {15710645},
journal = {Physics of Life Reviews},
keywords = {Auditory system,Music cognition,Music perception,Musical expectancy},
number = {3},
pages = {169--182},
publisher = {Elsevier B.V.},
title = {{Computational models of music perception and cognition II: Domain-specific music processing}},
url = {http://dx.doi.org/10.1016/j.plrev.2008.03.005},
volume = {5},
year = {2008}
}
@article{Roda2014,
abstract = {The important role of the valence and arousal dimensions in representing and recognizing affective qualities in music is well established. There is less evidence for the contribution of secondary dimensions such as potency, tension and energy. In particular, previous studies failed to find significant relations between computable musical features and affective dimensions other than valence and arousal. Here we present two experiments aiming at assessing how musical features, directly computable from complex audio excerpts, are related to secondary emotion dimensions. To this aim, we imposed some constraints on the musical features, namely modality and tempo, of the stimuli.The results show that although arousal and valence dominate for many musical features, it is possible to identify features, in particular Roughness, Loudness, and SpectralFlux, that are significantly related to the potency dimension. As far as we know, this is the first study that gained more insight into the affective potency in the music domain by using real music recordings and a computational approach.},
author = {Rod{\`{a}}, Antonio and Canazza, Sergio and {De Poli}, Giovanni},
doi = {10.1109/TAFFC.2014.2343222},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/06866179.pdf:pdf},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Music,affective dimensions,automated mood analysis,emotions,musical features,potency},
number = {4},
pages = {364--376},
publisher = {IEEE},
title = {{Clustering affective qualities of classical music: Beyond the valence-arousal plane}},
volume = {5},
year = {2014}
}
@article{Papiotis2014,
abstract = {In a musical ensemble such as a string quartet, the musicians interact and influence each other's actions in several aspects of the performance simultaneously in order to achieve a common aesthetic goal. In this article, we present and evaluate a computational approach for measuring the degree to which these interactions exist in a given performance. We recorded a number of string quartet exercises under two experimental conditions (solo and ensemble), acquiring both audio and bowing motion data. Numerical features in the form of time series were extracted from the data as performance descriptors representative of four distinct dimensions of the performance: Intonation, Dynamics, Timbre and Tempo. Four different interdependence estimation methods (two linear and two nonlinear) were applied to the extracted features in order to assess the overall level of interdependence between the four musicians. The obtained results suggest that it is possible to correctly discriminate between the two experimental conditions by quantifying interdependence between the musicians in each of the studied performance dimensions; the nonlinear methods appear to perform best for most of the numerical features tested. Moreover, by using the solo recordings as a reference to which the ensemble recordings are contrasted, it is feasible to compare the amount of interdependence that is established between the musicians in a given performance dimension across all exercises, and relate the results to the underlying goal of the exercise. We discuss our findings in the context of ensemble performance research, the current limitations of our approach, and the ways in which it can be expanded and consolidated. {\textcopyright} 2014 Papiotis, Marchini, Perez-carrillo and Maestre.},
author = {Papiotis, Panos and Marchini, Marco and Perez-Carrillo, Alfonso and Maestre, Esteban},
doi = {10.3389/fpsyg.2014.00963},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/fpsyg-05-00963.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Capture,Causality,Coupling,Ensemble,Granger,Information,Interdependence,Motion,Mutual,Nonlinear,Performance,Processing,Quartet,Signal,String},
number = {AUG},
pages = {1--17},
title = {{Measuring ensemble interdependence in a string quartet through analysis of multidimensional performance data}},
volume = {5},
year = {2014}
}
@article{Schimmack2000,
abstract = {The present article compares dimensional models of affect with each other. The article focuses on the pleasure - arousal model, the energetic and tense arousal model, and a three-dimensional model with separate pleasure-displeasure, awake - tiredness, and tension - relaxation dimensions. The results show that the three-dimensional model cannot be reduced to a two-dimensional model. Problems of the two-dimensional models' reductionism are discussed. We conclude that a three-dimensional description of affect is necessary. However, the three-dimensional model is not sufficient to account for all aspects of the structure of affect. Copyright {\textcopyright} 2000 John Wiley Sons, Ltd.},
author = {Schimmack, Ulrich and Grob, Alexander},
doi = {10.1002/1099-0984(200007/08)14:4<325::AID-PER380>3.0.CO;2-I},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/schimmack{\_}grob{\_}2000.pdf:pdf},
issn = {08902070},
journal = {European Journal of Personality},
number = {4},
pages = {325--345},
title = {{Dimensional models of core affect: A quantitative comparison by means of structural equation modeling}},
volume = {14},
year = {2000}
}
@article{Susini1999,
abstract = {One approach to improving sound quality is to create a preference map on the basis of several acoustic parameters relevant to auditory perception. The map is derived from several stages of subjective testing, acoustic analysis, and auditory modeling. The multidimensional scaling technique CLASCAL reveals common perceptual dimensions shared by sets of sounds samples, perceptual features specific to each sound, and the different subject classes among listeners. The listeners are asked to judge the degree of dissimilarity of all pairs of sounds on a continuous scale. The analysis gives a perceptual spatial representation of the sounds. From this analysis, acoustic and auditory modelling analyses can be performed to determine the stimulus parameters that are strongly correlated with different perceptual dimensions and, where possible, with the specific features. The next stage in the analysis involves determining the probability of one sound being preferred to another. An analysis of the data allows a projection of the structure of listeners' preferences onto the physical parameter space underlying the previously determined multidimensional perceptual space. In many cases, it is found that the physical parameters having the most effect on the listeners' preferences are dependent on the set of stimuli being compared. Furthermore, when one stimulus parameter is kept constant across trials, this may alter the effects of other parameters on the listeners' preferences. Therefore context effects must be taken into account in multidimensional sound quality analysis, particularly since the qualitative aspects of most sounds are clearly multidimensional.},
author = {Susini, Patrick and McAdams, Stephen and Winsberg, Suzanne},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/susini{\_}1999{\_}actaacustica.pdf:pdf},
issn = {00017884},
journal = {Acustica},
number = {5},
pages = {650--656},
title = {{A Multidimensional Technique for Sound Quality Assessment}},
volume = {85},
year = {1999}
}
@article{Cowen2020,
abstract = {What is the nature of the feelings evoked by music? We investigated how people represent the subjective experiences associated with Western and Chinese music and the form in which these representational processes are preserved across different cultural groups. US (n = 1,591) and Chinese (n = 1,258) participants listened to 2,168 music samples and reported on the specific feelings (e.g., "angry," "dreamy") or broad affective features (e.g., valence, arousal) that they made individuals feel. Using large-scale statistical tools, we uncovered 13 distinct types of subjective experience associated with music in both cultures. Specific feelings such as "triumphant" were better preserved across the 2 cultures than levels of valence and arousal, contrastingwith theoretical claims that valence and arousal are building blocks of subjective experience. This held true even for music selected on the basis of its valence and arousal levels and for traditional Chinese music. Furthermore, the feelings associated with music were found to occupy continuous gradients, contradicting discrete emotion theories. Our findings, visualized within an interactive map (https://www.ocf.berkeley.edu/ ∼acowen/music.HTML) reveal a complex, high-dimensional space of subjective experience associated with music in multiple cultures. These findings can inform inquiries ranging from the etiology of affective disorders to the neurological basis of emotion.},
author = {Cowen, Alan S. and Fang, Xia and Sauter, Disa and Keltner, Dacher},
doi = {10.1073/pnas.1910704117},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1924.full.pdf:pdf},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Affect,Culture,Emotion,Music,Semantic space},
number = {4},
pages = {1924--1934},
pmid = {31907316},
title = {{What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures}},
volume = {117},
year = {2020}
}
@article{Nielzen1982,
author = {Nielz{\'{e}}n, S{\"{o}}ren and Cesarec, Zvonimir},
doi = {10.1177/0305735682102002},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/0305735682102002.pdf:pdf},
issn = {17413087},
journal = {Psychology of Music},
number = {2},
pages = {7--17},
title = {{Emotional experience of music as a function of musical structure}},
volume = {10},
year = {1982}
}
@article{Drake1999,
abstract = {The concept of tempo often leads to confusion because it corresponds to both a musical (number of beats per minute) and psychological concept (perceived rate of events). The perceived tempo of various sequences (isochronous, irregular, accented, musical) varying in musical tempo was measured by asking twenty nonmusicians and twenty musicians to tap in time with sequences at the rate that they considered most appropriate. (I) Tapping rate varied with sequence tempo (listeners tapped slower with slower sequences), much slower than one tap per sounded event. No difference was observed between regular and irregular sequences, indicating the importance of event density. (2) Tapping rate was slower for accented than unaccented sequences, with tapping at higher hierarchical levels in the structured (accented) sequences. (3) For musical sequences, event density, musical tempo and personal tempo contributed significantly in explaining observed tapping rates. (4) Musicians tapped slower than nonmusicians, suggesting that they are able to perceptually organize events over a longer time span. The results do not support a direct mapping of musical to perceived tempo.},
author = {Drake, Carolyn and Gros, L. and Penel, Amandine},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/4PMU{\_}{\_}{\_}3.PDF:PDF},
journal = {Music, Mind, and Science},
pages = {190--203},
title = {{How fast is that music? The relation between physical and perceived tempo.}},
volume = {101},
year = {1999}
}
@article{Thompson2020,
abstract = {If the structure of language vocabularies mirrors the structure of natural divisions that are universally perceived, then the meanings of words in different languages should closely align. By contrast, if shared word meanings are a product of shared culture, history and geography, they may differ between languages in substantial but predictable ways. Here, we analysed the semantic neighbourhoods of 1,010 meanings in 41 languages. The most-aligned words were from semantic domains with high internal structure (number, quantity and kinship). Words denoting natural kinds, common actions and artefacts aligned much less well. Languages that are more geographically proximate, more historically related and/or spoken by more-similar cultures had more aligned word meanings. These results provide evidence that the meanings of common words vary in ways that reflect the culture, history and geography of their users.},
author = {Thompson, Bill and Roberts, Se{\'{a}}n G. and Lupyan, Gary},
doi = {10.1038/s41562-020-0924-8},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/thompson{\_}roberts{\_}lupyan{\_}2020.pdf:pdf},
issn = {23973374},
journal = {Nature Human Behaviour},
number = {10},
pages = {1029--1038},
pmid = {32778801},
title = {{Cultural influences on word meanings revealed through large-scale semantic alignment}},
volume = {4},
year = {2020}
}
@article{Madsen1997,
abstract = {This study compared emotional responses to an excerpt from Puccini's La Boh{\`{e}}me using a two-dimensional CRDI. The two-dimensional CRDI provides the possibility of using a computer screen via a mouse to indicate the interrelationship of two dimensions simultaneously. In the present investigation, 48 subjects were presented with two dimensions in order to assess perception of arousal, i.e., relaxing-exciting in relationship to affect, i.e., ugly-beautiful. Visual and temporal analyses of this 20-minute selection indicated that there is not a consequential difference between subjects' responses to the ugly-beautiful dimension throughout the excerpt that appears substantially different from previous aesthetic responses using this same excerpt. Furthermore, subjects' arousal responses (i.e., exciting vs. relaxing does not replicate the ugly-beautiful dimension and, therefore, adds important new information to overall analysis of emotional response to music. Responses to the exciting-relaxing dimension represent degrees of arousal that suggest one needs to be somewhat aroused in order to have an affective response. More importantly, the subtle relationships between arousal and affect provide important information for the music therapist.},
author = {Madsen, Clifford K.},
doi = {10.1093/jmt/34.3.187},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/34-3-187.pdf:pdf},
issn = {00222917},
journal = {Journal of Music Therapy},
number = {3},
pages = {187--199},
title = {{Emotional Response to Music as Measured by the Two-Dimensional CRDI}},
volume = {34},
year = {1997}
}
@article{Droe2006,
abstract = {One of the purposes of music education should be to increase knowledge of music styles and expand music preference. Students exiting a music program should have a broader basis for choosing music and perhaps an ability to enjoy more different styles of music than before they started. Research investigating factors that influence musical preference is reviewed. Factors include tempo, familiarity, teacher influence, peer influence, and modeling. Teachers should take the music preference of the students into consideration when designing lessons, choosing music, and delivering instruction. Knowledge of music preference and the factors that influence it are indispensable to music educators in discovering the appropriate rewards for their students: music that brings enjoyment to the student.},
author = {Droe, Kevin},
doi = {10.1177/87551233060240020103},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/87551233060240020103.pdf:pdf},
issn = {8755-1233},
journal = {Update: Applications of Research in Music Education},
number = {2},
pages = {23--32},
title = {{Music Preference and Music Education: A Review of Literature}},
volume = {24},
year = {2006}
}
@phdthesis{Vinovich1975,
author = {Vinovich, George Starr},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/VinovitchDissertation.pdf:pdf},
pages = {1--109},
school = {University of Southern California},
title = {{The Communicative Significance of Musical Affect in Eliciting Differential Perception, Cognition, and Emotion in Sound-Motion Media Messages}},
year = {1975}
}
@article{Witte2013,
author = {Witte, James and Kiss, Marissa and Lynn, Randy},
doi = {10.4324/9780203069769},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Digital{\_}Divide{\_}Highly{\_}developed.pdf:pdf},
isbn = {9780203069769},
journal = {The Digital Divide: The Internet and Social Inequality in International Perspective},
number = {2013},
pages = {67--84},
title = {{The Internet and social inequalities in the U.S}},
year = {2013}
}
@misc{Abdi2013a,
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Coputational Toxicology},
chapter = {Partial Le},
doi = {10.1007/978-1-62703-059-5},
editor = {Reisfeld, Brad and Mayeno, Arthur N.},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/C75-PLSC and PLSR.pdf:pdf},
isbn = {9781627030595},
pages = {1453--1454},
publisher = {Springer Science+Business Media, LLC},
title = {{Partial Least Squares Methods: Partial Least Squares Correlation and Partial Least Square Regression}},
volume = {II},
year = {2013}
}
@article{Droit-Volet2013,
abstract = {The present study used a temporal bisection task with short ({\textless}2 s) and long ({\textgreater}2 s) stimulus durations to investigate the effect on time estimation of several musical parameters associated with emotional changes in affective valence and arousal. In order to manipulate the positive and negative valence of music, Experiments 1 and 2 contrasted the effect of musical structure with pieces played normally and backwards, which were judged to be pleasant and unpleasant, respectively. This effect of valence was combined with a subjective arousal effect by changing the tempo of the musical pieces (fast vs. slow) (Experiment 1) or their instrumentation (orchestral vs. piano pieces). The musical pieces were indeed judged more arousing with a fast than with a slow tempo and with an orchestral than with a piano timbre. In Experiment 3, affective valence was also tested by contrasting the effect of tonal (pleasant) vs. atonal (unpleasant) versions of the same musical pieces. The results showed that the effect of tempo in music, associated with a subjective arousal effect, was the major factor that produced time distortions with time being judged longer for fast than for slow tempi. When the tempo was held constant, no significant effect of timbre on the time judgment was found although the orchestral music was judged to be more arousing than the piano music. Nevertheless, emotional valence did modulate the tempo effect on time perception, the pleasant music being judged shorter than the unpleasant music. {\textcopyright} 2013 Droit-Volet, Ramos, Bueno and Bigand.},
author = {Droit-Volet, Sylvie and Ramos, Danilo and Bueno, Jos{\'{e}} L.O. and Bigand, Emmanuel},
doi = {10.3389/fpsyg.2013.00417},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/fpsyg-04-00417.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Arousal,Emotion,Music,Time perception,Valence},
number = {JUL},
pages = {1--12},
title = {{Music, emotion, and time perception: The influence of subjective emotional valence and arousal?}},
volume = {4},
year = {2013}
}
@misc{Kennedy2013,
author = {Kennedy, Joyce and Kennedy, Michael and Rutherford-Johnson, Tim},
booktitle = {The Oxford Dictionary of Music},
edition = {6},
publisher = {Oxford University Press},
title = {harmony},
year = {2013}
}
@article{Siedenburg2016,
abstract = {A curious divide characterizes the usage of audio descriptors for timbre research in music information research (MIR) and music psychology. While MIR uses a multitude of audio descriptors for tasks such as automatic instrument classification, only a highly constrained set is used to describe the physical correlates of timbre perception in parts of music psychology. We argue that this gap is not coincidental and results from the differences in the two fields' methodologies, their epistemic groundwork, and research goals. This paper lays out perspectives on the emergence of the divide and reviews studies in both fields with regards to divergences in research methods and goals. We discuss new representations for spectro-temporal modulations in MIR and psychology, and compare approaches to spectral envelope description in depth. Finally, we will propose that the interdisciplinary discourse on the computational modelling of music requires negotiations about the roles of scientific evaluation criteria.},
author = {Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
doi = {10.1080/09298215.2015.1132737},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Siedenburg-fujinaga-mcadams.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
keywords = {audio analysis,evaluation,information retrieval,instrument classification,timbre perception},
number = {1},
pages = {27--41},
title = {{A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology}},
volume = {45},
year = {2016}
}
@incollection{Abdi2003,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia for research methods for the social sciences},
chapter = {Multivaria},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/C27-Multivariate analysis.pdf:pdf},
publisher = {Sage},
title = {{Multivariate Analysis}},
url = {https://personal.utdallas.edu/{~}herve/Abdi-MultivariateAnalysis-pretty.pdf},
year = {2003}
}
@book{Wallmark2018,
author = {Wallmark, Zachary and Kendall, Roger A.},
booktitle = {The Oxford Handbook of Timbre},
doi = {10.1093/oxfordhb/9780190637224.013.14},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/WallmarkKendall{\_}OH-2019.pdf:pdf},
isbn = {9780190637224},
keywords = {cognitive linguistics,conceptual metaphor,psychoacoustic methods,semantics,timbre perception},
number = {July},
pages = {1--37},
title = {{Describing Sound}},
year = {2018}
}
@article{Raman2020,
abstract = {Our initial aim in this study was to show that Western listeners can sort the music of 3 Western composers consistently on the basis of their compositional style. We found that they could, and proceeded to investigate what cues they might be using to accomplish that task, as well as whether their use of those cues was related to their level of musical training. In Experiment 1, we presented 21 excerpts from the keyboard music of Bach, Mozart, and Beethoven, each excerpt linked to an icon on the computer screen. Participants were to place the icons in different groups following the rule that the icons in one group could have been written by the same composer. First, they did a free sort in which they could form as many groups as they liked, and then we told them that there were just 3 composers, and they should make 3 groups in a constrained sort. In Experiment 1, the excerpts were produced with MIDI transcriptions of the scores, such that the composer's pitch and time information of the notes was preserved, but there was no variation in tempo, dynamics (loudness), or articulation (connectedness or separateness of notes in time). In spite of this simplification, listeners succeeded in clearly differentiating the composers in the constrained sort. In Experiment 2, we used more natural stimuli, 36 excerpts taken from recordings of the 3 composers by 4 pianists who had recorded substantial amounts of each: Arrau, Barenboim, Pir{\`{e}}s, and Richter. Here, the stimuli included all the expressive cues of a live performance, and in the constrained sort listeners were even better at categorizing the composers, with not very much difference between the categorizations of trained and untrained listeners. Their judgments were also strongly influenced by the pianists. Richter's performances of the 3 composers were clustered relatively close to the Mozart region of the solution, indicating their clarity and balance; in contrast, those of Barenboim were clustered in the Beethoven region, indicating their sumptuousness and passion. We used a relatively new approach to data analysis—DiSTATIS—which provided the possibility of projecting the sorting results viewed from various perspectives—composer, pianist, participant expertise—into the same space, giving a clearer picture of the results than a piecemeal account of those perspectives.},
author = {Raman, Rachna and Kriegsman, Michael A. and Abdi, Herv{\'{e}} and Tillmann, Barbara and Dowling, W. Jay},
doi = {10.1016/j.newideapsych.2019.100757},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Raman et al. - 2020 - Bach, Mozart, and Beethoven Sorting piano excerpts based on perceived similarity using DiSTATIS.pdf:pdf;:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Raman et al. - 2020 - Bach, Mozart, and Beethoven Sorting piano excerpts based on perceived similarity using DiSTATIS(2).pdf:pdf;:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Raman et al. - 2020 - Bach, Mozart, and Beethoven Sorting piano excerpts based on perceived similarity using DiSTATIS(3).pdf:pdf;:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/abdi.rkatd2020.BMB.pdf:pdf},
issn = {0732118X},
journal = {New Ideas in Psychology},
keywords = {Constrained sorting,Free sorting,Musical style,Musical training,Style perception},
number = {September 2019},
title = {{Bach, Mozart, and Beethoven: Sorting piano excerpts based on perceived similarity using DiSTATIS}},
volume = {57},
year = {2020}
}
@article{Zentner2008,
abstract = {One reason for the universal appeal of music lies in the emotional rewards that music offers to its listeners. But what makes these rewards so special? The authors addressed this question by progressively characterizing music-induced emotions in 4 interrelated studies. Studies 1 and 2 (n = 354) were conducted to compile a list of music-relevant emotion terms and to study the frequency of both felt and perceived emotions across 5 groups of listeners with distinct music preferences. Emotional responses varied greatly according to musical genre and type of response (felt vs. perceived). Study 3 (n = 801)-a field study carried out during a music festival-examined the structure of music-induced emotions via confirmatory factor analysis of emotion ratings, resulting in a 9-factorial model of music-induced emotions. Study 4 (n = 238) replicated this model and found that it accounted for music-elicited emotions better than the basic emotion and dimensional emotion models. A domain-specific device to measure musically induced emotions is introduced-the Geneva Emotional Music Scale. {\textcopyright} 2008 American Psychological Association.},
author = {Zentner, Marcel and Grandjean, Didier and Scherer, Klaus R.},
doi = {10.1037/1528-3542.8.4.494},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Zentneretal2008.pdf:pdf},
issn = {15283542},
journal = {Emotion},
keywords = {emotion,emotion induction,feeling,music,music preferences},
number = {4},
pages = {494--521},
pmid = {18729581},
title = {{Emotions Evoked by the Sound of Music: Characterization, Classification, and Measurement}},
volume = {8},
year = {2008}
}
@article{AsteriosZacharakis2015,
abstract = {THE CURRENT STUDY EXPANDS OUR PREVIOUS work on interlanguage musical timbre semantics by examining the relationship between semantics and per- ception of timbre. Following Zacharakis, Pastiadis, and Reiss (2014), a pairwise dissimilarity listening test involv- ing participants from two separate linguistic groups (Greek and English) was conducted. Subsequent multidi- mensional scaling analysis produced a 3Dperceptual tim- bre space for each language. The comparison between perceptual spaces suggested that timbre perception is unaffected by native language. Additionally, comparisons between semantic and perceptual spaces revealed sub- stantial similarities which suggest that verbal descriptions can convey a considerable amount ofperceptual informa- tion. The previouslydetermined semantic labels ‘‘auditory texture'' and ‘‘luminance'' featured the highest associa- tions with perceptual dimensions for both languages. ‘‘Auditory mass'' failed to show any strong correlations. Acoustic analysis identified energy distribution of har- monic partials, spectral detail, temporal/spectrotemporal characteristics and the fundamental frequency as the most salient acoustic correlates ofperceptual dimensions.},
author = {{Asterios Zacharakis} and {Konstantinos Pastiadis} and Reiss, Joshua D.},
doi = {10.1525/MP.2015.32.4.394},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/mp.2015.32.4.394.pdf:pdf},
journal = {Music Perception: An Interdisciplinary Journal},
number = {4},
pages = {394--412},
title = {{An Interlanguage Unification of Musical Timbre: Bridging Semantic, Perceptual, and Acoustic Dimensions}},
volume = {32},
year = {2015}
}
@article{Berry2011,
abstract = {Permutation tests are a paradox of old and new. Permutation tests pre-date most traditional parametric statistics, but only recently have become part of the mainstream discussion regarding statistical testing. Permutation tests follow a permutation or 'conditional on errors' model whereby a test statistic is computed on the observed data, then (1) the data are permuted over all possible arrangements of the data-an exact permutation test; (2) the data are used to calculate the exact moments of the permutation distribution-a moment approximation permutation test; or (3) the data are permuted over a subset of all possible arrangements of the data-a resampling approximation permutation test. The earliest permutation tests date from the 1920s, but it was not until the advent of modern day computing that permutation tests became a practical alternative to parametric statistical tests. In recent years, permutation analogs of existing statistical tests have been developed. These permutation tests provide noteworthy advantages over their parametric counterparts for small samples and populations, or when distributional assumptions cannot be met. Unique permutation tests have also been developed that allow for the use of Euclidean distance rather than the squared Euclidean distance that is typically employed in parametric tests. This overview provides a chronology of the development of permutation tests accompanied by a discussion of the advances in computing that made permutation tests feasible. Attention is paid to the important differences between 'population models' and 'permutation models', and between tests based on Euclidean and squared Euclidean distances. WIREs Comp Stat 2011 3 527-542 DOI: 10.1002/wics.177 For further resources related to this article, please visit the WIREs website. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Inc..},
author = {Berry, Kenneth J. and Johnston, Janis E. and Mielke, Paul W.},
doi = {10.1002/wics.177},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Permutation{\_}Berry.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Euclidean distance,Exact tests,Permutation tests,Resampling},
number = {6},
pages = {527--542},
title = {{Permutation methods}},
volume = {3},
year = {2011}
}
@article{Hevner1936,
author = {Hevner, Kate},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1415746.pdf:pdf},
journal = {The American Journal of Psychology},
number = {2},
pages = {246--268},
title = {{Experimental Studies of the Elements of Expression in Music}},
url = {https://www.jstor.org/stable/1415746},
volume = {48},
year = {1936}
}
@article{Ares2010,
abstract = {Eight chocolate milk desserts with different formulation were evaluated by two groups of consumers. Fifty consumers evaluated the samples and indicated their overall liking and answered a CATA question. Meanwhile, 40 consumers elicited up to four words to describe the desserts and completed a projective mapping task. Projective mapping and the check-all-that-apply question provided very similar sensory profiles for the evaluated milk desserts. Differences in the sensory characteristics of the samples were explained by differences in their formulations, which suggest the validity of the sensory profiles given by consumers. Projective mapping and the CATA question consisted on valuable tools to understand their perception of the sensory and hedonic characteristics of the desserts. These methodologies could consist on useful and interesting complimentary techniques to trained assessors' data, being CATA question easier to understand and less time consuming for consumers. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Ares, Gast{\'{o}}n and Deliza, Rosires and Barreiro, Cecilia and Gim{\'{e}}nez, Ana and G{\'{a}}mbaro, Adriana},
doi = {10.1016/j.foodqual.2009.10.006},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S0950329309001785-main.pdf:pdf},
issn = {09503293},
journal = {Food Quality and Preference},
keywords = {Check-all-that-apply,Consumer research,Hierarchical multiple factor analysis,Milk desserts,Multiple factor analysis,Projective mapping,Sensory profiling},
number = {4},
pages = {417--426},
publisher = {Elsevier Ltd},
title = {{Comparison of two sensory profiling techniques based on consumer perception}},
url = {http://dx.doi.org/10.1016/j.foodqual.2009.10.006},
volume = {21},
year = {2010}
}
@article{Wedin1972,
author = {Wedin, Lage},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/STM1972Wedin.pdf:pdf},
journal = {The Psychological Laboratories},
number = {349},
pages = {1--17},
title = {{Evaluation of a Three-Dimensional Model of Emotional Expression in Music}},
volume = {54},
year = {1972}
}
@article{Wallmark2013,
author = {Wallmark, Zachary},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Big{\_}Data{\_}and{\_}Musicology{\_}New{\_}Methods{\_}New.pdf:pdf},
number = {1},
pages = {1--7},
title = {{Big Data and Musicology: New Methods, New Questions}},
volume = {1},
year = {2013}
}
@article{Abdi2007d,
abstract = {P CA is obtained by performing the eigen-decomposition of a ma- trix. This matrix can be a correlation matrix (ie, the variables to be analyzed are centered and normalized), a covariance matrix (ie, the variables are centered but not normalized), or a cross- product matrix (ie, the},
author = {Abdi, Herv{\'{e}}},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Abdi-MDS2007-pretty.pdf:pdf},
isbn = {9781412916110},
journal = {Encyclopedia of Measurement and Statistics},
pages = {598--605},
title = {{Metric Multidimensional Scaling (MDS): Analyzing Distance Matrices Multidimensional Scaling : Eigen-analysis of a distance matrix}},
year = {2007}
}
@article{LeMenestrel2007,
author = {{Le Menestrel}, Sara},
doi = {10.1353/scu.2007.0032},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/a704-5647-5245-r47p.pdf:pdf},
issn = {15341488},
journal = {Southern Cultures},
number = {3},
pages = {87--105},
title = {{The color of music}},
volume = {13},
year = {2007}
}
@article{Wallmark2019a,
author = {Wallmark, Zachary},
doi = {10.1177/2059204319846617},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Wallmark{\_}MNS{\_}2019.pdf:pdf},
issn = {2059-2043},
journal = {Music {\&} Science},
keywords = {2 august 2018,5 april 2019,acceptance date,automaticity,cross-modal correspondences,described using adjectives,musical timbre is commonly,semantics,speeded classification,stroop,submission date,timbre},
number = {May},
pages = {205920431984661},
title = {{Semantic Crosstalk in Timbre Perception}},
volume = {2},
year = {2019}
}
@article{Istok2009,
abstract = {We explored the content and structure of the cognitive, knowledge-based concept underlying aesthetic responses to music. To this aim, we asked 290 Finnish students to verbally associate the aesthetic value of music and to write down a list of appropriate adjectives within a given time limit. No music was presented during the task. In addition, information about participants' musical background was collected. A variety of analysis techniques was used to determine the key results of our study. The adjective "beautiful" proved to be the core item of the concept under question. Interestingly, the adjective "touching" was often listed together with "beautiful". In addition, we found music-specific vocabulary as well as adjectives related to emotions and mood states indicating that affective processes are an essential part of aesthetic responses to music. Differences between music experts and laymen as well as between female and male participants were found for a number of adjectives. These findings suggest the existence of a common conceptual space underlying aesthetic responses to music, which partly can be modified by the level of musical expertise and gender. {\textcopyright} 2009 by ESCOM European Society for the Cognitive Sciences of Music.},
author = {Ist{\'{o}}k, Eva and Brattico, Elvira and Jacobsen, Thomas and Krohn, Kaisu and M{\"{u}}ller, Mira and Tervaniemi, Mari},
doi = {10.1177/102986490901300201},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/102986490901300201.pdf:pdf},
issn = {10298649},
journal = {Musicae Scientiae},
keywords = {Aesthetics,Beauty,Concept,Emotions,Verbal association},
number = {2},
pages = {183--206},
title = {{Aesthetic responses to music: A questionnaire study}},
volume = {13},
year = {2009}
}
@article{Ares2014,
abstract = {One of the most important steps of new product development process is product optimization, which aims at identifying consumers' ideal products and directions for product reformulation. The present work proposes the application of a penalty analysis based on consumer responses to CATA questions to identify drivers of liking and directions for product reformulation. Two studies were conducted in which 74 and 119 consumers evaluated a set of samples (5 apples and 8 yogurts) using a check-all-that-apply question related to sensory characteristics and were also asked to check all the terms they considered appropriate to describe their ideal product. Data were analyzed by counting the number of consumers who did not check an attribute as they did for their ideal product, and its associated mean drop. A dummy variable transformation approach was proposed to make linear regression models between CATA terms and overall liking scores using Partial Least Squares (PLS). Juiciness, sweetness, apple flavor, firmness and crispiness were the most relevant attributes for consumers in the apple study. Meanwhile, in the yogurt study smoothness, homogeneity and creaminess were the main drivers of liking and were responsible for the highest penalization on overall liking (more than 1 in the 9-point hedonic scale). PLS regression enabled the identification of the attributes which deviation from the ideal caused a significant decrease in overall liking. Penalty analysis on CATA questions proved to be a simple and useful approach to identify drivers of liking and directions for improving the products in both studies. Advantages and disadvantages of this approach are discussed, as well as directions for further research. {\textcopyright} 2013 Elsevier Ltd.},
author = {Ares, Gast{\'{o}}n and Dauber, Cecilia and Fern{\'{a}}ndez, Elisa and Gim{\'{e}}nez, Ana and Varela, Paula},
doi = {10.1016/j.foodqual.2013.05.014},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S0950329313000955-main.pdf:pdf},
issn = {09503293},
journal = {Food Quality and Preference},
keywords = {Apples,CATA,Consumer studies,Product optimization,Yogurt},
pages = {65--76},
publisher = {Elsevier Ltd},
title = {{Penalty analysis based on CATA questions to identify drivers of liking and directions for product reformulation}},
url = {http://dx.doi.org/10.1016/j.foodqual.2013.05.014},
volume = {32},
year = {2014}
}
@article{McAdams1999,
abstract = {The perceptual salience of several outstanding features of quasiharmonic, time-variant spectra was investigated in musical instrument sounds. Spectral analyses of sounds from seven musical instruments (clarinet, flute, oboe, trumpet, violin, harpsichord, and marimba) produced time-varying harmonic amplitude and frequency data. Six basic data simplifications and five combinations of them were applied to the reference tones: amplitude-variation smoothing, coherent variation of amplitudes over time, spectral-envelope smoothing, forced harmonic-frequency variation, frequency-variation smoothing, and harmonic-frequency flattening. Listeners were asked to discriminate sounds resynthesized with simplified data from reference sounds resynthesized with the full data. Averaged over the seven instruments, the discrimination was very good for spectral envelope smoothing and amplitude envelope coherence, but was moderate to poor in decreasing order for forced harmonic frequency variation, frequency variation smoothing, frequency flattening, and amplitude variation smoothing. Discrimination of combinations of simplifications was equivalent to that of the most potent constituent simplification. Objective measurements were made on the spectral data for harmonic amplitude, harmonic frequency, and spectral centroid changes resulting from simplifications. These measures were found to correlate well with discrimination results, indicating that listeners have access to a relatively fine-grained sensory representation of musical instrument sounds.},
author = {McAdams, Stephen and Beauchamp, James W. and Meneguzzi, Suzanna},
doi = {10.1121/1.426277},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/mcadams{\_}1999{\_}jasa.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {882--897},
pmid = {9972573},
title = {{Discrimination of musical instrument sounds resynthesized with simplified spectrotemporal parameters}},
volume = {105},
year = {1999}
}
@book{Goos1999,
author = {Goos, G. and Hartmanis, J. and Leeuwen, J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/2004{\_}Book{\_}ComputerMusicModelingAndRetrie.pdf:pdf},
isbn = {3540666664},
issn = {16113349},
pages = {437--445},
title = {{Lecture Notes in Computer Science}},
volume = {1716},
year = {1999}
}
@book{Benzecri1973a,
address = {Paris},
author = {Benzécri, Jean-Paul},
publisher = {Dunod},
title = {{L' analyse des données 2, L'analyse de correspondances.}},
year = {1973}
}
@article{Mori2017,
abstract = {People sometimes experience a strong emotional response to artworks. Previous studies have demonstrated that the peak emotional experience of chills (goose bumps or shivers) when listening to music involves psychophysiological arousal and a rewarding effect. However, many aspects of peak emotion are still not understood. The current research takes a new perspective of peak emotional response of tears (weeping, lump in the throat). A psychophysiological experiment showed that self-reported chills increased electrodermal activity and subjective arousal whereas tears produced slow respiration during heartbeat acceleration, although both chills and tears induced pleasure and deep breathing. A song that induced chills was perceived as being both happy and sad whereas a song that induced tears was perceived as sad. A tear-eliciting song was perceived as calmer than a chill-eliciting song. These results show that tears involve pleasure from sadness and that they are psychophysiologically calming; thus, psychophysiological responses permit the distinction between chills and tears. Because tears may have a cathartic effect, the functional significance of chills and tears seems to be different. We believe that the distinction of two types of peak emotions is theoretically relevant and further study of tears would contribute to more understanding of human peak emotional response.},
author = {Mori, Kazuma and Iwanaga, Makoto},
doi = {10.1038/srep46063},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/srep46063.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {April},
pages = {1--13},
pmid = {28387335},
publisher = {Nature Publishing Group},
title = {{Two types of peak emotional responses to music: The psychophysiology of chills and tears}},
volume = {7},
year = {2017}
}
@misc{Abdi2010d,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Encyclopedia of Research Design},
chapter = {Correspond},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/C69-Correspondence analysis.pdf:pdf},
publisher = {Sage},
title = {{Correspondence Analysis}},
year = {2010}
}
@article{Katz1933,
abstract = {The degree of agreement among the students in assigning characteristics from a list of 84 adjectives to different races seemed too great to be the result solely of the students' contacts with members of those races. Individual experience may have entered into a student's judgment, but it probably did so to confirm the original stereotype which he had learned. Because human beings from time to time exhibit all kinds of behavior he could find confirmation of his views. By omitting cases which contradict the stereotype, the individual becomes convinced from association with a race that its members are just the kind of people he always thought they were. The manner in which public and private attitudes are bound up together was shown in the order of the 10 racial and national groups as determined by the definiteness with which students assigned characteristics to them. The definiteness of the stereotyped picture of a race, however, had little relation to the prejudice exhibited against that race. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1933 American Psychological Association.},
author = {Katz, D. and Braly, K.},
doi = {10.1037/h0074049},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/KatzBraly1933.pdf:pdf},
issn = {0096851X},
journal = {Journal of Abnormal and Social Psychology},
keywords = {PREJUDICE, RACE,RACE ATTITUDE VS. INFORMATION, STEREOTYPES,SOCIAL FUNCTIONS OF THE INDIVIDUAL,STEREOTYPE, RACE},
number = {3},
pages = {280--290},
title = {{Racial stereotypes of one hundred college students}},
volume = {28},
year = {1933}
}
@article{Osgood1955,
abstract = {Two factor analytic studies of meaningful judgments based upon the same sample of 50 bipolar descriptive scales are reported. Both analyses reveal three major connotative factors: evaluation, potency, and activity. These factors appear to be independent dimensions of the semantic space within which the meanings of concepts may be specified. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1955 American Psychological Association.},
author = {Osgood, Charles E. and Suci, George J.},
doi = {10.1037/h0043965},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Osgood1955.pdf:pdf},
issn = {00221015},
journal = {Journal of Experimental Psychology},
keywords = {FACTOR ANALYSIS, OF MEANING,LANGUAGE {\&} COMMUNICATION,MEANING {\&} COMMUNICATION, FACTOR ANALYSIS OF},
number = {5},
pages = {325--338},
pmid = {13271697},
title = {{Factor analysis of meaning}},
volume = {50},
year = {1955}
}
@article{Wallmark2019,
abstract = {What does the common descriptive lexicon for instrumental sound tell us about how we conceptualize musical timbre? Perceptual studies have revealed a number of verbal attributes that reliably map onto timbral qualities, but the conventions of timbre description in spoken and written discourse remain poorly understood. Books on orchestration provide a valuable source of natural language about instrumental timbre. This article uses methods from corpus linguistics to explore the semantic features of timbre through a quantitative analysis of 11 orchestration treatises and manuals. Findings reveal a relatively constrained vocabulary for timbre: about 50 adjectives account for half of all descriptions in the corpus. The timbre lexicon can be categorized according to affect, matter, metaphor, mimesis, action, acoustics, and onomatopoeia, and further reduced to three latent conceptual dimensions, which are labeled and discussed. Descriptive patterns vary systematically by instrument and instrument family, suggesting certain regularities and consistencies to timbre description in the orchestral tradition. This study helps test the long-held assumption that conventions of timbre description are vague and unsystematic, and offers a cognitive linguistic account of the timbre-language connection.},
author = {Wallmark, Zachary},
doi = {10.1177/0305735618768102},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/A{\_}corpus{\_}analysis{\_}of{\_}timbre{\_}semantics{\_}in{\_}orchestra.pdf:pdf},
issn = {17413087},
journal = {Psychology of Music},
keywords = {Timbre,corpus methods,musicology,orchestration,semantics,text analysis},
number = {4},
pages = {585--605},
title = {{A corpus analysis of timbre semantics in orchestration treatises}},
volume = {47},
year = {2019}
}
@book{Benzecri1973,
address = {Paris},
author = {Benzécri, Jean-Paul},
pages = {615},
publisher = {Dunod},
title = {{L'analyse des données.}},
year = {1973}
}
@article{Reilly2012,
abstract = {Cognitive science has a rich history of interest in the ways that languages represent abstract and concrete concepts (e.g., idea vs. dog). Until recently, this focus has centered largely on aspects of word meaning and semantic representation. However, recent corpora analyses have demonstrated that abstract and concrete words are also marked by phonological, orthographic, and morphological differences. These regularities in sound-meaning correspondence potentially allow listeners to infer certain aspects of semantics directly from word form. We investigated this relationship between form and meaning in a series of four experiments. In Experiments 1-2 we examined the role of metalinguistic knowledge in semantic decision by asking participants to make semantic judgments for aurally presented nonwords selectively varied by specific acoustic and phonetic parameters. Participants consistently associated increased word length and diminished wordlikeness with abstract concepts. In Experiment 3, participants completed a semantic decision task (i.e., abstract or concrete) for real words varied by length and concreteness. Participants were more likely to misclassify longer, inflected words (e.g., "apartment") as abstract and shorter uninflected abstract words (e.g., "fate") as concrete. In Experiment 4, we used a multiple regression to predict trial level naming data from a large corpus of nouns which revealed significant interaction effects between concreteness and word form. Together these results provide converging evidence for the hypothesis that listeners map sound to meaning through a non-arbitrary process using prior knowledge about statistical regularities in the surface forms of words. {\textcopyright} 2012 Reilly et al.},
author = {Reilly, Jamie and Westbury, Chris and Kean, Jacob and Peelle, Jonathan E.},
doi = {10.1371/journal.pone.0042286},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/pone.0042286.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pmid = {22879931},
title = {{Arbitrary symbolism in natural language revisited: When word forms carry meaning}},
volume = {7},
year = {2012}
}
@article{Yang2012,
abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research. {\textcopyright} 2012 ACM 2157-6904/2012/05-ART40 {\$}10.00.},
author = {Yang, Yi Hsuan and Chen, Homer H.},
doi = {10.1145/2168752.2168754},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/2168752.2168754.pdf:pdf},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Music emotion recognition},
number = {3},
title = {{Machine recognition of music emotion: A review}},
volume = {3},
year = {2012}
}
@book{Borg2005,
author = {Borg, Ingwer and Groenen, Patrick J.F.},
booktitle = {Springer},
edition = {2},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Borg-Groenen2005{\_}Book{\_}ModernMultidimensionalScaling.pdf:pdf},
isbn = {9780387251509},
issn = {87566648},
pages = {1--614},
publisher = {Springer Science+Business Media, Inc.},
title = {{Modern Multidimensional Scaling}},
volume = {36},
year = {2005}
}
@book{Greenacre1984,
author = {Greenacre, Michael J.},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/[Greenacre 1984] Theory and applications of CA.pdf:pdf},
isbn = {0-12-299050-1},
pages = {1--376},
publisher = {Academic Press},
title = {{Theory and Applications of Correspondence Analysis}},
year = {1984}
}
@book{Juslin2010,
abstract = {Music's ability to express and arouse emotions is a mystery that has fascinated both experts and laymen at least since ancient Greece. The Handbook of Music and Emotion offers an up-to-date account of this vibrant domain. It provides comprehensive coverage of the many approaches that may be said to define the field of music and emotion, in all its breadth and depth. The first section offers multi-disciplinary perspectives on musical emotions from philosophy, musicology, psychology, neurobiology, anthropology, and sociology. The second section features methodologically-oriented chapters on the measurement of emotions via different channels (e.g., self report, psychophysiology, neuroimaging). Sections three and four address how emotion enters into different aspects of musical behavior, both the making of music and its consumption. Section five covers developmental, personality, and social factors. Section six describes the most important applications involving the relationship between music and emotion. In a final commentary, the editors comment on the history of the field, summarize the current state of affairs, as well as propose future directions for the field. The only book of its kind, the Handbook of Music and Emotion will fascinate music psychologists, musicologists, music educators, philosophers, and others with an interest in music and emotion (e.g., in marketing, health, engineering, film, and the game industry). It will be a valuable resource for established researchers in the field, a developmental aid for early-career researchers and postgraduate research students, and a compendium to assist students at various levels. In addition, as with its predecessor, it will also attract interest from practicing musicians and lay readers fascinated by music and emotion. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
address = {New York, NY, US},
booktitle = {Handbook of music and emotion: Theory, research, applications.},
editor = {Juslin, Patrik N and Sloboda, John A},
isbn = {0-19-923014-5 (Hardcover); 978-0-19-923014-3 (Hardcover)},
keywords = {Emotions,History,Human Development,Measurement,Music,Personality,Psychosocial Factors},
pages = {xiv, 975--xiv, 975},
publisher = {Oxford University Press},
series = {Series in affective science.},
title = {{Handbook of music and emotion: Theory, research, applications.}},
year = {2010}
}
@article{Juslin2008,
abstract = {Research indicates that people value music primarily because of the emotions it evokes. Yet, the notion of musical emotions remains controversial, and researchers have so far been unable to offer a satisfactory account of such emotions. We argue that the study of musical emotions has suffered from a neglect of underlying mechanisms. Specifically, researchers have studied musical emotions without regard to how they were evoked, or have assumed that the emotions must be based on the "default" mechanism for emotion induction, a cognitive appraisal. Here, we present a novel theoretical framework featuring six additional mechanisms through which music listening may induce emotions: (1) brain stem reflexes, (2) evaluative conditioning, (3) emotional contagion, (4) visual imagery, (5) episodic memory, and (6) musical expectancy. We propose that these mechanisms differ regarding such characteristics as their information focus, ontogenetic development, key brain regions, cultural impact, induction speed, degree of volitional influence, modularity, and dependence on musical structure. By synthesizing theory and findings from different domains, we are able to provide the first set of hypotheses that can help researchers to distinguish among the mechanisms. We show that failure to control for the underlying mechanism may lead to inconsistent or non-interpretable findings. Thus, we argue that the new framework may guide future research and help to resolve previous disagreements in the field. We conclude that music evokes emotions through mechanisms that are not unique to music, and that the study of musical emotions could benefit the emotion field as a whole by providing novel paradigms for emotion induction. {\textcopyright} 2008 Cambridge University Press.},
author = {Juslin, Patrik N. and V{\"{a}}stfj{\"{a}}ll, Daniel},
doi = {10.1017/S0140525X08005293},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/emotional-responses-underlying-mechanisms.pdf:pdf},
issn = {0140525X},
journal = {Behavioral and Brain Sciences},
keywords = {Affect,Arousal,Brain,Emotion,Induction,Mechanism,Memory,Music,Theory},
number = {5},
pages = {559--621},
pmid = {18826699},
title = {{Emotional responses to music: The need to consider underlying mechanisms}},
volume = {31},
year = {2008}
}
@article{Abdi2010f,
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/A77-PCA.pdf:pdf},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
number = {August},
pages = {1--16},
title = {{Principal component analysis Tutorial Review}},
volume = {2},
year = {2010}
}
@article{Hesterberg2011,
abstract = {This article provides an introduction to the bootstrap. The bootstrap provides statistical inferences-standard error and bias estimates, confidence intervals, and hypothesis tests-without assumptions such as Normal distributions or equal variances. As such, bootstrap methods can be remarkably more accurate than classical inferences based on Normal or t distributions. The bootstrap uses the same basic procedure regardless of the statistic being calculated, without requiring the use of application-specific formulae. This article may provide two big surprises for many readers. The first is that the bootstrap shows that common t confidence intervals are woefully inaccurate when populations are skewed, with one-sided coverage levels off by factors of two or more, even for very large samples. The second is that the number of bootstrap samples required is much larger than generally realized. WIREs Comp Stat 2011 3 497-526 DOI: 10.1002/wics.182 For further resources related to this article, please visit the WIREs website Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Inc.},
author = {Hesterberg, Tim},
doi = {10.1002/wics.182},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Bootstrap{\_}Hesterberg.pdf:pdf},
isbn = {9781118596333},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bias,Inference,Permutation tests,Resampling,Standard error},
number = {6},
pages = {497--526},
title = {{Bootstrap}},
volume = {3},
year = {2011}
}
@article{Caclin2005,
abstract = {Timbre spaces represent the organization of perceptual distances, as measured with dissimilarity ratings, among tones equated for pitch, loudness, and perceived duration. A number of potential acoustic correlates of timbre-space dimensions have been proposed in the psychoacoustic literature, including attack time, spectral centroid, spectral flux, and spectrum fine structure. The experiments reported here were designed as direct tests of the perceptual relevance of these acoustical parameters for timbre dissimilarity judgments. Listeners presented with carefully controlled synthetic tones use attack time, spectral centroid, and spectrum fine structure in dissimilarity rating experiments. These parameters thus appear as major determinants of timbre. However, spectral flux appears as a less salient timbre parameter, its salience depending on the number of other dimensions varying concurrently in the stimulus set. Dissimilarity ratings were analyzed with two different multidimensional scaling models (CLASCAL and CONSCAL), the latter providing psychophysical functions constrained by the physical parameters. Their complementarity is discussed.},
author = {Caclin, Anne and McAdams, Stephen and Smith, Bennett K. and Winsberg, Suzanne},
doi = {10.1121/1.1929229},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/caclin{\_}2005{\_}jasa{\_}0.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {471--482},
pmid = {16119366},
title = {{Acoustic correlates of timbre space dimensions: A confirmatory study using synthetic tones}},
volume = {118},
year = {2005}
}
@article{Paquette2013,
abstract = {The Musical Emotional Bursts (MEB) consist of 80 brief musical executions expressing basic emotional states (happiness, sadness and fear) and neutrality. These musical bursts were designed to be the musical analog of the Montreal Affective Voices (MAV)-a set of brief non-verbal affective vocalizations portraying different basic emotions. The MEB consist of short (mean duration: 1.6 s) improvisations on a given emotion or of imitations of a given MAV stimulus, played on a violin (10 stimuli × 4 [3 emotions + neutral]), or a clarinet (10 stimuli × 4 [3 emotions + neutral]). The MEB arguably represent a primitive form of music emotional expression, just like the MAV represent a primitive form of vocal, non-linguistic emotional expression. To create the MEB, stimuli were recorded from 10 violinists and 10 clarinetists, and then evaluated by 60 participants. Participants evaluated 240 stimuli [30 stimuli × 4 (3 emotions + neutral) × 2 instruments] by performing either a forced-choice emotion categorization task, a valence rating task or an arousal rating task (20 subjects per task); 40 MAVs were also used in the same session with similar task instructions. Recognition accuracy of emotional categories expressed by the MEB (n:80) was lower than for the MAVs but still very high with an average percent correct recognition score of 80.4{\%}. Highest recognition accuracies were obtained for happy clarinet (92.0{\%}) and fearful or sad violin (88.0{\%} each) MEB stimuli. The MEB can be used to compare the cerebral processing of emotional expressions in music and vocal communication, or used for testing affective perception in patients with communication problems. {\textcopyright} 2013 Paquette, Peretz and Belin.},
author = {Paquette, S{\'{e}}bastien and Peretz, Isabelle and Belin, Pascal},
doi = {10.3389/fpsyg.2013.00509},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/fpsyg-04-00509.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Auditory stimuli,Emotion,Music,Voices},
number = {AUG},
pages = {1--7},
title = {{The "Musical Emotional Bursts": A validated set of musical affect bursts to investigate auditory affective processing}},
volume = {4},
year = {2013}
}
@incollection{Abdi2007b,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
chapter = {Distance},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/C49-Distance.pdf:pdf},
pages = {39--85},
publisher = {Sage},
title = {{Distance}},
volume = {307},
year = {2007}
}
@article{Fornari2009,
abstract = {In the study of music emotions, Valence is usually referred to as one of the dimensions of the circumplex model of emotions that describes music appraisal of happiness, whose scale goes from sad to happy. Nevertheless, re- lated literature shows that Valence is known as being particularly difficult to be predicted by a computational model. As Valence is a contextual music feature, it is assumed here that its prediction should also require contextual music de- scriptors in its predicting model. This work describes the usage of eight contex- tual (also known as higher-level) descriptors, previously developed by us, to calculate happiness in music. Each of these descriptors was independently tested using the correlation coefficient of its prediction with the mean rating of Valence, reckoned by thirty-five listeners, over a piece of music. Following, a linear model using this eight descriptors was created and the result of its predic- tion, for the same piece of music, is described and compared with two other computational models from the literature, designed for the dynamic prediction of music emotion. Finally it is proposed here an initial investigation on the ef- fects of expressive performance and musical structure on the prediction of Valence. Our descriptors are then separated in two groups: performance and structural, where, with each group, we built a linear model. The prediction of Valence given by these two models, over two other pieces of music, are here compared with the correspondent listeners' mean rating of Valence, and the achieved results are depicted, described and discussed.},
author = {Fornari, Jos{\'{e}} and Eerola, Tuomas},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/Fornari-Eerola2009{\_}Chapter{\_}ThePursuitOfHappinessInMusicRe.pdf:pdf},
journal = {Computer Music Modeling and Retrieval},
keywords = {music cognition,music emotion,music information retrieval},
pages = {119--133},
title = {{The Pursuit of Happiness in Music : Retrieving Valence}},
year = {2009}
}
@article{Kamenetsky1997,
abstract = {Adults (N = 96) with little or no training in music heard one of four possible MIDI versions of each of four musical excerpts. The four versions of each excerpt included one with unvarying tempo and dynamics, one with variations in tempo only, one with variations in dynamics only, and one with variations in tempo and dynamics. Participants rated each excerpt on a 7-point scale for likeability and emotional expressiveness. Variations in dynamics resulted in higher ratings on both measures but variations in tempo had no such effect. In general, women rated the musical excerpts as more emotionally expressive and more likeable than did men. Finally, musical preferences were highly correlated with ratings of emotional expressiveness. {\textcopyright} 1997 by the Society for Research in Psychology of Music and Music Education.},
author = {Kamenetsky, Stuart B. and Hill, David S. and Trehub, Sandra E.},
doi = {10.1177/0305735697252005},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/0305735697252005.pdf:pdf},
issn = {17413087},
journal = {Psychology of Music},
number = {2},
pages = {149--160},
title = {{Effect of tempo and dynamics on the perception of emotion in music}},
volume = {25},
year = {1997}
}
@article{Krishnan2011,
abstract = {Partial Least Squares (PLS) methods are particularly suited to the analysis of relationships between measures of brain activity and of behavior or experimental design. In neuroimaging, PLS refers to two related methods: (1) symmetric PLS or Partial Least Squares Correlation (PLSC), and (2) asymmetric PLS or Partial Least Squares Regression (PLSR). The most popular (by far) version of PLS for neuroimaging is PLSC. It exists in several varieties based on the type of data that are related to brain activity: behavior PLSC analyzes the relationship between brain activity and behavioral data, task PLSC analyzes how brain activity relates to pre-defined categories or experimental design, seed PLSC analyzes the pattern of connectivity between brain regions, and multi-block or multi-table PLSC integrates one or more of these varieties in a common analysis. PLSR, in contrast to PLSC, is a predictive technique which, typically, predicts behavior (or design) from brain activity. For both PLS methods, statistical inferences are implemented using cross-validation techniques to identify significant patterns of voxel activation. This paper presents both PLS methods and illustrates them with small numerical examples and typical applications in neuroimaging. {\textcopyright} 2010 Elsevier Inc.},
author = {Krishnan, Anjali and Williams, Lynne J. and McIntosh, Anthony Randal and Abdi, Herv{\'{e}}},
doi = {10.1016/j.neuroimage.2010.07.034},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/A81-PLS method for neuroimaging-a tutorial and review.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Asymmetric PLS,Barycentric discriminant analysis,Behavior PLS,Canonical variate analysis,Co-inertia analysis,Common factor analysis,Multi-block PLS,Multi-table PLS,Multiple factor analysis,PLS,Partial least squares correlation,Partial least squares path modeling,Partial least squares regression,STATIS,Seed PLS,Symmetric PLS,Task PLS},
number = {2},
pages = {455--475},
title = {{Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review}},
volume = {56},
year = {2011}
}
@article{Kopacz2005,
abstract = {The purpose of this scientific study was to determine how personality traits, as classified by Cattell, influence preferences regarding musical elements. The subject group consisted of 145 students, male and female, chosen at random from different Polish universities. For the purpose of determining their personality traits the participants completed the 16PF Questionnaire (Cattell, Saunders, {\&} Stice, 1957; Russel {\&} Karol, 1993), in its Polish adaptation by Choynowski (Nowakowska, 1970). The participants' musical preferences were determined by their completing a Questionnaire of Musical Preferences (specifically created for the purposes of this research), in which respondents indicated their favorite piece of music. Next, on the basis of the Questionnaire of Musical Preferences, a list of the works of music chosen by the participants was compiled. All pieces were collected on CDs and analyzed to separate out their basic musical elements. The statistical analysis shows that some personality traits: Liveliness (Factor F), Social Boldness (Factor H), Vigilance (Factor L), Openness to Change (Factor 01), Extraversion (a general factor) have an influence on preferences regarding musical elements. Important in the subjects' musical preferences were found to be those musical elements having stimulative value and the ability to regulate the need for stimulation. These are: tempo, rhythm in relation to metrical basis, number of melodic themes, sound voluminosity, and meter. {\textcopyright} 2005 by the American Music Therapy Association.},
author = {Kopacz, Malgorzata},
doi = {10.1093/jmt/42.3.216},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/42-3-216.pdf:pdf},
issn = {00222917},
journal = {Journal of Music Therapy},
number = {3},
pages = {216--239},
title = {{Personality and music preferences: The influence of personality traits on preferences regarding musical elements}},
volume = {42},
year = {2005}
}
@article{Hernandez-Ruiz2020,
abstract = {The purpose of this study was to compare the effectiveness of and preference for different auditory stimuli on mindfulness meditation in musicians. A second purpose was to compare musician responses with non-musician responses from a previous study. A repeated-measures design exposed participants to four auditory stimuli of increased complexity. Participants (N = 49) were undergraduate musicians with limited mindfulness experience. Data included absorption in music, mindfulness, and preference and usefulness of auditory stimuli. A repeated-measures analysis of covariance, with absorption of music as a covariate, found no significant differences between stimuli on mindfulness meditation according to musicians. Friedman's analyses of variance indicated that musician rankings of usefulness and preference were significantly different among conditions. Both musicians and non-musicians ranked Melody and Harmony conditions as most preferred and most useful for mindfulness meditation. A mixed effects model with both groups indicated not only a significant effect of auditory stimuli on mindfulness but also interaction due to group status. A significant result was only obtained when the covariate was not considered. Absorption in music scores between groups was significantly higher for musicians than non-musicians. These outcomes support the hypothesis that absorption in music and music expertise may mediate the effect of a music intervention. Clinical implications are discussed.},
author = {Hernandez-Ruiz, Eugenia and Dvorak, Abbey L. and Weingarten, Kevin},
doi = {10.1177/0305735620901338},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/0305735620901338.pdf:pdf},
issn = {17413087},
journal = {Psychology of Music},
keywords = {absorption in music,complexity,course-based undergraduate research,mindfulness,music},
title = {{Music stimuli in mindfulness meditation: Comparison of musician and non-musician responses}},
year = {2020}
}
@article{Vastfjall2001,
abstract = {This article reviews research showing that music can alter peoples' moods and emotions. The so called “musical mood induction procedure” (MMIP) relies on music to produce changes in experienced affective processes. The fact that music can have this effect on subjective experience has been utilized to study the effect of mood on cognitive processes and behavior by a large number of researchers in social, clinical, and personality psychology. This extensive body of literature, while little known among music psychologists, is likely to further help music psychologists understand affective responses to music. With this in mind, the present article aims at providing an extensive review of the methodology behind a number of studies using the MMIP. The effectiveness of music as a mood-inducing stimulus is discussed in terms of self-reports, physiological, and behavioral indices. The discussion focuses on how findings from the MMIP literature may extend into current research and debate on the complex interplay of music and emotional responses.},
author = {V{\"{a}}stfj{\"{a}}ll, Daniel},
doi = {10.1177/10298649020050s107},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/10298649020050s107.pdf:pdf},
issn = {1029-8649},
journal = {Musicae Scientiae},
number = {1{\_}suppl},
pages = {173--211},
title = {{Emotion induction through music: A review of the musical mood induction procedure}},
volume = {5},
year = {2001}
}
@article{Juslin2004,
abstract = {In this article, we provide an up-to-date overview of theory and research concerning expression, perception, and induction of emotion in music. We also provide a critique of this research, noting that previous studies have tended to neglect the social context of music listening. The most likely reason for this neglect, we argue, is that that most research on musical emotion has, implicitly or explicitly, taken the perspective of the musician in understanding responses to music. In contrast, we argue that a promising avenue toward a better understanding of emotional responses to music involves diary and questionnaire studies of how ordinary listeners actually use music in everyday life contexts. Accordingly, we present findings from an exploratory questionnaire study featuring 141 music listeners (between 17 and 74 years of age) that offers some novel insights. The results provide preliminary estimates of the occurrence of various emotions in listening to music, as well as clues to how music is used by listeners in a number of different emotional ways in various life contexts. These results confirm that emotion is strongly related to most people's primary motives for listening to music. {\textcopyright} 2004, Taylor {\&} Francis Group, LLC.},
author = {Juslin, Patrik N. and Laukka, Petri},
doi = {10.1080/0929821042000317813},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/JuslinLaukka2004.pdf:pdf},
issn = {17445027},
journal = {Journal of New Music Research},
number = {3},
pages = {217--238},
title = {{Expression, Perception, and Induction of Musical Emotions: A Review and a Questionnaire Study of Everyday Listening}},
volume = {33},
year = {2004}
}
@article{Coombs1956,
author = {Coombs, Clyde H. and Milholland, J. E. and Womer, F. B.},
doi = {10.1177/001316445601600102},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/001316445601600102.pdf:pdf},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {13--37},
title = {{The assessment of partial knowledge}},
volume = {16},
year = {1956}
}
@book{Ragnedda2013,
editor = {Ragnedda, M. and Muschert, G. W.},
publisher = {Taylor {\&} Francis Group},
title = {{The digital divide: The internet and social inequality in internation perspective}},
year = {2013}
}
@article{BrunerII1990,
author = {{Bruner II}, Gordon C.},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/002224299005400408.pdf:pdf},
journal = {Journal of Marketing},
number = {October},
pages = {94--104},
title = {{Music, Mood, and Marketing}},
year = {1990}
}
@article{Warrenburg2020,
abstract = {WHEN DESIGNING A NEW STUDY REGARDING HOW music can portray and elicit emotion, one of the most crucial design decisions involves choosing the best stimuli. Every researcher must find musical samples that are able to capture an emotional state, are appropriate lengths, and have minimal potential for biasing participants. Researchers have often utilized musical excerpts that have previously been used by other scholars, but the appropriate musical choices depend on the specific goals of the study in question and will likely change among various research designs. The intention of this paper is to examine how musical stimuli have been selected in a sample of 306 research articles dating from 1928 through 2018. Analyses are presented regarding the designated emotions, how the stimuli were selected, the durations of the stimuli, whether the stimuli are excerpts from a longer work, and whether the passages have been used in studies about perceived or induced emotion. The results suggest that the literature relies on nine emotional terms, focuses more on perceived emotion than on induced emotion, and contains mostly short musical stimuli. I suggest that some of the inconclusive results from previous reviews may be due to the inconsistent use of emotion terms throughout the music community.},
author = {Warrenburg, Lindsay A.},
doi = {10.1525/MP.2020.37.3.240},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/mp{\_}2020{\_}37{\_}3{\_}240.pdf:pdf},
issn = {15338312},
journal = {Music Perception},
keywords = {Database,Emotion,Music,Review,Stimuli},
number = {3},
pages = {240--258},
title = {{Choosing the right tune: A review of music stimuli used in emotion research}},
volume = {37},
year = {2020}
}
@article{Cheever2018,
abstract = {The National Institutes of Health and John F. Kennedy Center for the Performing Arts convened a panel of experts to discuss the current state of research on music and the brain. The panel generated research recommendations to accelerate the study of music's effects on the brain and the implications for human health. The National Institutes of Health and John F. Kennedy Center for the Performing Arts convened a panel of experts to discuss the current state of research on music and the brain. The panel generated research recommendations to accelerate the study of music's effects on the brain and the implications for human health.},
author = {Cheever, Thomas and Taylor, Anna and Finkelstein, Robert and Edwards, Emmeline and Thomas, Laura and Bradt, Joke and Holochwost, Steven J. and Johnson, Julene K. and Limb, Charles and Patel, Aniruddh D. and Tottenham, Nim and Iyengar, Sunil and Rutter, Deborah and Fleming, Ren{\'{e}}e and Collins, Francis S.},
doi = {10.1016/j.neuron.2018.02.004},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S089662731830103X-main.pdf:pdf},
issn = {10974199},
journal = {Neuron},
number = {6},
pages = {1214--1218},
pmid = {29566791},
title = {{NIH/Kennedy Center Workshop on Music and the Brain: Finding Harmony}},
volume = {97},
year = {2018}
}
@article{Howard2007a,
author = {Howard, David M. and Disley, Alastair C. and Hunt, Andrew D.},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/mus-07-004.pdf:pdf},
journal = {19th INTERNATIONAL CONGRESS ON ACOUSTICS},
number = {September},
pages = {2--7},
title = {{Timbral Adjectives for the Control of a Music Synthesizer}},
year = {2007}
}
@incollection{Meyners2014,
abstract = {{\textcopyright} 2014 by Taylor {\&} Francis Group, LLC. A check-all-that-apply (CATA, often also known as choose-all-that-apply) question is a question format that has been used in recent years to obtain rapid product profiles from consumers. Consumers are presented with a list of attributes and asked to indicate which words or phrases appropriately describe their experience with the sample being evaluated. The terms might include sensory attributes, as well as hedonic responses, emotional responses, purchase intentions, potential applications, product positioning, or other terms that the consumer might associate with the sample.},
author = {Meyners, Michael and Castura, John},
booktitle = {Novel Techniques in Sensory Characterization and Consumer Profiling},
chapter = {10},
doi = {10.1201/b16853-12},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/CATA{\_}chapterPREPRINT.pdf:pdf},
number = {May 2014},
pages = {271--306},
publisher = {CRC Press/Taylor {\&} Francis},
title = {{Check-All-That-Apply Questions}},
year = {2014}
}
@article{Abdi2013,
abstract = {Multiple factor analysis (MFA, also called multiple factorial analysis) is an extension of principal component analysis (PCA) tailored to handle multiple data tables that measure sets of variables collected on the same observations, or, alternatively, (in dual-MFA) multiple data tables where the same variables are measured on different sets of observations. MFA proceeds in two steps: First it computes a PCA of each data table and 'normalizes' each data table by dividing all its elements by the first singular value obtained from its PCA. Second, all the normalized data tables are aggregated into a grand data table that is analyzed via a (non-normalized) PCA that gives a set of factor scores for the observations and loadings for the variables. In addition, MFA provides for each data table a set of partial factor scores for the observations that reflects the specific 'view-point' of this data table. Interestingly, the common factor scores could be obtained by replacing the original normalized data tables by the normalized factor scores obtained from the PCA of each of these tables. In this article, we present MFA, review recent extensions, and illustrate it with a detailed example. {\textcopyright} 2013 Wiley Periodicals, Inc.},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J. and Valentin, Domininique},
doi = {10.1002/wics.1246},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/A101-Multiple factor analysis- principal component analysis for multitable and multiblock data sets.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Barycentric discriminant analysis (BADA),Consensus PCA,Generalized Procrustes analysis (GPA),Generalized singular value decomposition,INDSCAL,Multiblock PCA,Multiblock barycentric discriminant analysis (MUDI,Multiblock correspondence analysis,Multiple factor analysis (MFA),Multiple factor analysis barycentric discriminant,Multiple factorial analysis,Multitable PCA,Principal component analysis,STATIS},
number = {2},
pages = {149--179},
title = {{Multiple factor analysis: Principal component analysis for multitable and multiblock data sets}},
volume = {5},
year = {2013}
}
@article{Panda2020a,
abstract = {The design of meaningful audio features is a key need to advance the state-of-the-art in Music Emotion Recognition (MER). This work presents a survey on the existing emotionally-relevant computational audio features, supported by the music psychology literature on the relations between eight musical dimensions (melody, harmony, rhythm, dynamics, tone color, expressivity, texture and form) and specific emotions. Based on this review, current gaps and needs are identified and strategies for future research on feature engineering for MER are proposed, namely ideas for computational audio features that capture elements of musical form, texture and expressivity that should be further researched. Finally, although the focus of this article is on classical feature engineering methodologies (based on handcrafted features), perspectives on deep learning-based approaches are discussed.},
author = {Panda, Renato and Malheiro, Ricardo Manuel and Paiva, Rui Pedro},
doi = {10.1109/taffc.2020.3032373},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/09229494.pdf:pdf},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
number = {c},
pages = {1--1},
title = {{Audio Features for Music Emotion Recognition: a Survey}},
volume = {3045},
year = {2020}
}
@article{Koelsch2018,
abstract = {Does our understanding of the human brain remain incomplete without a proper understanding of how the brain processes music? Here, the author makes a passionate plea for the use of music in the investigation of human emotion and its brain correlates, arguing that music can change activity in all brain structures associated with emotions, which has important implications on how we understand human emotions and their disorders and how we can make better use of beneficial effects of music in therapy. Does our understanding of the human brain remain incomplete without a proper understanding of how the brain processes music? Here, the author makes a passionate plea for the use of music in the investigation of human emotion and its brain correlates, arguing that music can change activity in all brain structures associated with emotions, which has important implications on how we understand human emotions and their disorders and how we can make better use of beneficial effects of music in therapy.},
author = {Koelsch, Stefan},
doi = {10.1016/j.neuron.2018.04.029},
file = {:C$\backslash$:/Users/Brendon/Documents/Mendeley Desktop/1-s2.0-S0896627318303374-main.pdf:pdf},
issn = {10974199},
journal = {Neuron},
keywords = {amygdala,attachment-related emotions,brain,emotion,hippocampus,music,nucleus accumbens,reward system},
number = {6},
pages = {1075--1079},
pmid = {29953870},
publisher = {Elsevier Inc.},
title = {{Investigating the Neural Encoding of Emotion with Music}},
url = {https://doi.org/10.1016/j.neuron.2018.04.029},
volume = {98},
year = {2018}
}
